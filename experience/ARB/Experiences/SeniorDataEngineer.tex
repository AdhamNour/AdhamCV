\item \textbf{Senior Data Engineer} \hfill Al-Rajhi Bank (ARB), Riyadh, Saudi Arabia (\textit{January 2025 â€“ Present})
\input{experience/ARB/OrganizasionIntroduction.tex}
\begin{itemize}
    \item \textbf{Modern Data Stack Proof of Concept (PoC):}
    \begin{itemize}
        \item \textbf{Architecture Modernization:} Spearheading a strategic migration from legacy Informatica and Oracle DB silos to a decoupled \textbf{Medallion Architecture}.
        \item \textbf{Infrastructure Evolution:} Architected a storage-compute decoupling strategy using \textbf{Apache Iceberg} and \textbf{OCI S3-Compatible Object Storage}, projected to significantly reduce licensing overhead.
        \item \textbf{Airflow-dbt Implementation:} Replaced rigid legacy mappings with \textbf{Apache Airflow} and \textbf{dbt} for 5+ critical workflows, enhancing version control and developer velocity.
        \item \textbf{High-Performance Analytics:} Integrated \textbf{Trino} as the primary query engine to facilitate low-latency interactive analytics over Iceberg tables for enterprise BI tools.
    \end{itemize}

    \item \textbf{SIMAH Reporting}
    \begin{itemize}
        \item \textbf{Regulatory Compliance:} Engineered end-to-end data pipelines to aggregate and transform credit risk data for submission to \textbf{SIMAH}, Saudi Arabia's leading credit bureau.
        \item \textbf{Performance Optimization:} Optimized ETL workflows to reduce data processing time by \textbf{30\%}, ensuring timely report generation ahead of regulatory deadlines.
    \end{itemize}
    \item Designed and deployed \textbf{real-time} data ingestion pipelines using \textbf{Apache Kafka, Kafka Connect, Kafka Streams and kSQL} to stream data from core banking, microservices and CRM systems into the data lake, defining data contracts and transformation logic to ensure data quality, scalability, and reliable downstream analytics.
    \item Engineered \textbf{Python}-based automation tools to streamline \textbf{Informatica DEI} troubleshooting and schema reconciliation between \textbf{ERwin} models and physical databases, reducing manual validation and alignment effort by up to 60\% and improving platform reliability.
    \item Led System Integration Testing (SIT) and User Acceptance Testing (UAT) for critical data pipelines and BI reports, coordinating across engineering, QA, and business stakeholders to validate correctness, performance, and regulatory compliance. Implemented monitoring and structured feedback loops during testing phases, resolving 95\% of issues prior to production and ensuring stable post-deployment operations.

    % \item \textbf{Data Engineering Operations \& Automation:}
    % \begin{itemize}
    %     \item \textbf{Real-Time Streaming:} Implemented stream-processing pipelines using \textbf{Kafka/Redpanda} for real-time data ingestion across core banking applications.
    %     \item \textbf{Python Automation Suites:} Developed custom tooling to automate Informatica DEI troubleshooting and environment object comparisons, reducing manual validation time by \textbf{40\%}.
    %     \item \textbf{Schema Integrity:} Engineered an automated reconciliation tool between \textbf{ERwin data models} and physical schemas, achieving a \textbf{50\%} reduction in alignment efforts.
    %     \item \textbf{Agile Leadership:} Drive delivery within \textbf{Agile squads} using JIRA/Confluence to translate complex business metrics into scalable dimensional models (Star/Snowflake).
    % \end{itemize}
\end{itemize}